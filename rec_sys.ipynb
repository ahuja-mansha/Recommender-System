import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import numpy as np

# (user_id, item_id, rating)
interactions = [
    (0, 10, 5.0),
    (0, 11, 4.0),
    (1, 10, 3.0),
    (1, 12, 4.5),
    (2, 13, 5.0)
]

num_users = 5
num_items = 20

# Item metadata (used only by LLM method)
item_texts = {
    10: "A thrilling sci-fi adventure",
    11: "Romantic drama set in Paris",
    12: "Action-packed superhero film",
    13: "Heartwarming family movie"
}


class MatrixFactorization(nn.Module):
    def __init__(self, num_users, num_items, latent_dim=16):
        super().__init__()
        self.user_emb = nn.Embedding(num_users, latent_dim)
        self.item_emb = nn.Embedding(num_items, latent_dim)

    def forward(self, user_ids, item_ids):
        u = self.user_emb(user_ids)
        i = self.item_emb(item_ids)
        return (u * i).sum(dim=1)

# Training CF
cf_model = MatrixFactorization(num_users, num_items, latent_dim=16)
optimizer = torch.optim.Adam(cf_model.parameters(), lr=1e-2)
loss_fn = nn.MSELoss()

user_ids = torch.tensor([u for u,_,_ in interactions], dtype=torch.long)
item_ids = torch.tensor([i for _,i,_ in interactions], dtype=torch.long)
ratings = torch.tensor([r for _,_,r in interactions], dtype=torch.float)

for epoch in range(200):
    preds = cf_model(user_ids, item_ids)
    loss = loss_fn(preds, ratings)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# --------------------------
# 3. LLM-based Item Embeddings
# --------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
llm_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2").to(device)

def get_llm_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = llm_model(**inputs)
        emb = outputs.last_hidden_state.mean(dim=1)
    return emb.squeeze(0).cpu()

item_llm_emb = torch.zeros((num_items, llm_model.config.hidden_size))
for item_id, text in item_texts.items():
    item_llm_emb[item_id] = get_llm_embedding(text)

# Derive user embeddings as average of their rated items (weighted by rating)
def build_user_llm_embedding(user_id):
    rated_items = [(i, r) for (u,i,r) in interactions if u==user_id]
    if not rated_items:
        return torch.zeros(llm_model.config.hidden_size)
    embs = torch.stack([item_llm_emb[i]*r for i,r in rated_items])
    return embs.mean(dim=0)

user_llm_emb = torch.stack([build_user_llm_embedding(u) for u in range(num_users)])

def recommend_cf(user_id, top_k=5):
    items = torch.arange(num_items)
    user_tensor = torch.tensor([user_id]*num_items)
    scores = cf_model(user_tensor, items)
    top_items = torch.topk(scores, k=top_k).indices.tolist()
    return top_items

def recommend_llm(user_id, top_k=5):
    user_vec = user_llm_emb[user_id]
    sims = F.cosine_similarity(user_vec.unsqueeze(0), item_llm_emb)
    top_items = torch.topk(sims, k=top_k).indices.tolist()
    return top_items

for uid in range(3):
    print(f"\nUser {uid}")
    print("CF Recommendations:", recommend_cf(uid))
    print("LLM Recommendations:", recommend_llm(uid))
